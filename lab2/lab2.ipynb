{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# **Laboratorium 2**: Tensorflow Playground\n*Wojciech Gierszal, 141037*\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Import\nW całym laboratorium będziemy korzystać z biblioteki numpy.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n## **Zadanie 1.**\nZaimplementuj prosty neuron realizujący dystans L2 dla wektora dwuelementowego (funkcja realizująca obliczenia w przód – feed-forward).\n\n---\n\nZa pomocą równania $f(x, W) = \\sum_{i = 1}^{n}(W \\cdot x)_i^2$ można stworzyć następującą funkcję:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def calculate_l2(inputs, weights):\n    return np.sum(np.dot(weights, inputs)**2)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Według przykładu podanego w laboratorium, możemy zapisać, że dla\n$\n\\mathbf{W} = \\begin{bmatrix}\n0.1 & 0.5 \\\\\n-0.3 & 0.8 \\\\\n\\end{bmatrix},\\quad\n\\mathbf{x} = \\begin{bmatrix}\n0.2 \\\\\n0.4 \\\\\n\\end{bmatrix}\n$\nwynikiem działania funkcji powinna być liczba $0.116$. Sprawdźmy zatem, czy otrzymujemy taki wynik:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "W = np.array([[0.1, 0.5], [-0.3, 0.8]])\nx = np.array([[0.2], [0.4]])\n\ncalculate_l2(x, W)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.11600000000000005"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Jak widać, wynik funkcji jest prawidłowy.\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Dla następujących zadań, jeżeli nie określono inaczej, używane są podane parametry sieci:\n\n![Default Parameters](https://github.com/Gierszu/SSN/blob/master/lab2/images/param.png?raw=true)\n\nParametr \"Noise\" ustawiany jest albo na 0 albo na 25, a \"Features\" posiadają tylko \"X1\" oraz \"X2\":\n\n![Zero noise](https://github.com/Gierszu/SSN/blob/master/lab2/images/param_nonoise.png?raw=true)\n![25 noise](https://github.com/Gierszu/SSN/blob/master/lab2/images/param_noise.png?raw=true)\n![Features](https://github.com/Gierszu/SSN/blob/master/lab2/images/param_features.png?raw=true)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Zadanie 2.**\nDla wszystkich rodzajów danych (z wyjątkiem „Spiral”), sprawdź:\n\n### **2a.** \nCzy jest możliwe rozwiązanie zadania bez żadnej warstwy ukrytej. Badanie należy wykonać dla zaszumienia (Noise) równego 0 oraz 25. \n\n---\n\n* **Circle**: \n    Nie jest możliwe poprawne rozwiązanie zadania, niezależnie od poziomu zaszumienia.\n* **Exclusive Or**:\n    Nie jest możliwe poprawne rozwiązanie zadania, niezależnie od poziomu zaszumienia.\n* **Gaussian**:\n    Tak, dla poziomu zaszumienia równego 0, możliwe jest uzyskanie stuprocentowej pewności przy rozwiązaniu zadania.  \n    Dla poziomu zaszumienia równego 25, możliwe jest uzyskanie rozwiązania o bardzo wysokiej pewności:  \n    ![25 Noise Gaussian](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_gaussian.png?raw=true)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### **2b.** \nCzy jest możliwe rozwiązanie zadania z jedną warstwą ukrytą. Jaka jest minimalna liczba neuronów danej warstwy, która realizuje zadanie klasyfikacji? Badanie należy wykonać dla zaszumienia (Noise) równego 0 oraz 25. \n\n---\n\n* **Circle**: \n    Tak, ale wymagane są co najmniej trzy neurony warstwy ukrytej. Działa zarówno dla zaszumienia 0 i 25.\n* **Exclusive Or**:\n    Poprawną realizację określiłbym dla co najmniej czterech neuronów:  \n    ![XOR 4 Neurons](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor4.png?raw=true)\n    ![XOR 4 Neurons with noise](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor4noise.png?raw=true)  \n    Ciekawe wyniki pojawiały się natomiast dla dwóch neuronów. Sieć próbowała rozwiązać ten problem na zupełnie różne sposoby:  \n    ![XOR 2 Neurons 1](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor21.png?raw=true)\n    ![XOR 2 Neurons 2](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor22.png?raw=true)\n* **Gaussian**:\n    Tak, można uzyskać poprawną realizację nawet bez warstwy ukrytej, z warstwą ukrytą możemy zaobserowować jedynie niewielki wzrost dokładności.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### **2c.** oraz **2d.**\nCzy zastosowanie regularyzacji „L2” statystycznie polepsza jakość klasyfikacji, bądź przyśpiesza czas trenowania? Należy zestawić wyniki z tymi otrzymanymi w podpunkcie b.\n\n---\n\nDla wszystkich przypadków parametr \"Regularization rate\" ustawiłem na 0.01. Pozostawienie tej wartości na zerze skutkowałoby brakiem jakiegokolwiek wpływu regularyzacji na trening, jak opisano pod [tym linkiem](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda).\n\nPoniższe tabele dotyczą, w kolejności od lewej, typów danych sferycznych, exclusive or oraz gaussian.\n\n|         | \\| |                       | Circle              |                      |                    | \\| |         | \\| |                       | XOR                 |                      |                    | \\| |         | \\| |                       | Gaussian            |                      |                    |\n|---------|----|-----------------------|---------------------|----------------------|--------------------|----|---------|----|-----------------------|---------------------|----------------------|--------------------|----|---------|----|-----------------------|---------------------|----------------------|--------------------|\n|  **Nr** | \\| | **Ilość epok bez L2** | **Ilość epok z L2** | **Test loss bez L2** | **Test loss z L2** | \\| |  **Nr** | \\| | **Ilość epok bez L2** | **Ilość epok z L2** | **Test loss bez L2** | **Test loss z L2** | \\| |  **Nr** | \\| | **Ilość epok bez L2** | **Ilość epok z L2** | **Test loss bez L2** | **Test loss z L2** |\n|  **1**  | \\| |          132          |         139         |         0.132        |        0.128       | \\| |  **1**  | \\| |          207          |          73         |         0.108        |        0.116       | \\| |  **1**  | \\| |           10          |          12         |         0.055        |        0.054       |\n|  **2**  | \\| |          101          |         124         |         0.099        |        0.105       | \\| |  **2**  | \\| |           63          |          64         |         0.112        |        0.117       | \\| |  **2**  | \\| |           9           |          12         |         0.042        |        0.046       |\n|  **3**  | \\| |           97          |         167         |         0.114        |        0.113       | \\| |  **3**  | \\| |          170          |          95         |         0.109        |        0.116       | \\| |  **3**  | \\| |           10          |          10         |         0.059        |        0.058       |\n|  **4**  | \\| |          103          |         103         |         0.111        |        0.114       | \\| |  **4**  | \\| |          120          |         178         |         0.107        |        0.112       | \\| |  **4**  | \\| |           11          |          12         |         0.068        |        0.061       |\n|  **5**  | \\| |          123          |         129         |         0.113        |        0.117       | \\| |  **5**  | \\| |          101          |         116         |         0.116        |        0.116       | \\| |  **5**  | \\| |           10          |          13         |         0.063        |        0.072       |\n|  **6**  | \\| |          107          |         115         |         0.148        |        0.146       | \\| |  **6**  | \\| |           48          |          56         |         0.152        |        0.128       | \\| |  **6**  | \\| |           10          |          9          |         0.053        |        0.054       |\n|  **7**  | \\| |           80          |         112         |         0.103        |        0.106       | \\| |  **7**  | \\| |           91          |          50         |         0.103        |        0.113       | \\| |  **7**  | \\| |           9           |          12         |         0.044        |        0.045       |\n|  **8**  | \\| |           82          |         145         |         0.115        |        0.114       | \\| |  **8**  | \\| |          191          |         163         |         0.147        |        0.152       | \\| |  **8**  | \\| |           7           |          13         |         0.054        |        0.053       |\n|  **9**  | \\| |           76          |         113         |         0.105        |        0.112       | \\| |  **9**  | \\| |           62          |          66         |         0.135        |        0.134       | \\| |  **9**  | \\| |           10          |          13         |         0.038        |        0.037       |\n|  **10** | \\| |          106          |         107         |         0.102        |        0.107       | \\| |  **10** | \\| |           93          |         140         |         0.142        |        0.141       | \\| |  **10** | \\| |           11          |          13         |         0.037        |        0.035       |\n| **Śr.** | \\| |        **101**        |       **125**       |       **0.114**      |      **0.116**     | \\| | **Śr.** | \\| |        **115**        |       **100**       |       **0.123**      |      **0.125**     | \\| | **Śr.** | \\| |         **10**        |        **12**       |       **0.051**      |      **0.052**     |\n| **Std** | \\| |         **18**        |        **20**       |       **0.015**      |      **0.012**     | \\| | **Std** | \\| |         **56**        |        **47**       |       **0.019**      |      **0.014**     | \\| | **Std** | \\| |         **1**         |        **1**        |       **0.011**      |      **0.011**     |\n\nNa podstawie tych danych można dojść do wniosku, że w ogólności zastosowanie regularyzacji L2 powiększa czas nauki oraz powiększa \"test loss\". Zmniejsza natomiast odchylenie standardowe parametru \"test loss\".",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **Zadanie 3.** oraz **Zadanie 4.**\nSprawdź, która funkcja aktywacji daje statystycznie najmniejszą karę dla nauczonej sieci oraz która daje najszybszą naukę. Analizę należy wykonać zastosowując dane sferyczne.\n\n---\n\nW celu sprawdzenia, nie stosowano regularyzacji, a wszystkie pomiary wykonywane były w oparciu o ten sam zbiór danych.\n\n|         | \\| |          | Czas nauki |             | \\| |           | Test loss |             |\n|---------|----|----------|------------|-------------|----|-----------|-----------|-------------|\n|  **Nr** | \\| | **ReLU** |  **Tanh**  | **Sigmoid** | \\| |  **ReLU** |  **Tanh** | **Sigmoid** |\n|  **1**  | \\| |    83    |     63     |     217     | \\| |   0.138   |   0.118   |    0.119    |\n|  **2**  | \\| |    81    |     68     |     379     | \\| |   0.127   |    0.12   |    0.117    |\n|  **3**  | \\| |    54    |     67     |     237     | \\| |   0.117   |   0.119   |    0.122    |\n|  **4**  | \\| |    43    |     102    |     390     | \\| |   0.121   |   0.117   |    0.112    |\n|  **5**  | \\| |    39    |     93     |     334     | \\| |   0.116   |   0.116   |    0.114    |\n|  **6**  | \\| |    51    |     75     |     281     | \\| |   0.129   |   0.118   |    0.117    |\n|  **7**  | \\| |    94    |     92     |     293     | \\| |   0.148   |   0.116   |    0.114    |\n|  **8**  | \\| |    133   |     104    |     307     | \\| |   0.131   |   0.119   |    0.113    |\n|  **9**  | \\| |    46    |     76     |     228     | \\| |   0.125   |   0.114   |    0.113    |\n|  **10** | \\| |    45    |     107    |     304     | \\| |   0.121   |   0.115   |    0.112    |\n| **Śr.** | \\| |  **67**  |   **85**   |   **297**   | \\| | **0.127** | **0.117** |  **0.115**  |\n| **Std** | \\| |  **30**  |   **17**   |    **60**   | \\| | **0.010** | **0.002** |  **0.003**  |\n\nNa podstawie tych danych można dojść do wniosku, że dla danych sferycznych najmniejszy parametr \"test loss\" daje funkcja aktywacji typu \"Sigmoid\", natomiast nauka z tą funkcją trwa najdłużej. Najkrócej zaś trwa nauka z funkcją aktywaci \"ReLU\", natomiast daje największy \"test loss\".",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Wnioski**\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Laboratorium pozwoliło mi zapoznać się w graficzny sposób z metodami działania algorytmów sztucznej inteligencji. Obserwowanie ich działania na stronie [Tensorflow Playground](https://playground.tensorflow.org) znacznie zbliżyło mnie do tematyki sztucznych sieci neuronowych, zwłaszcza że nie miałem wcześniej z nimi styczności. Po raz pierwszy również użyłem oprogramowania JupyterLab oraz języka Markdown.\n\nZobaczyłem, że zastosowanie regularyzacji L2 zarówno zwiększa czas uczenia, jak i zwiększa straty. Jest to spowodowane tym, że do funkcji obliczania strat znacznie zwiększany jest wpływ pojedyńczych danych, które są zdecydowanie źle sklasyfikowane, co wymusza potrzebę tworzenia lepszych sztucznych sieci neuronowych. Zobaczyłem też, że w zależności od potrzeb, możemy stosować różne funkcje aktywacji neuronów. Jeśli sieć jest wysoce skomplikowana i mamy małe zasoby obliczeniowe, a dokładność sieci nie musi być na najwyższym poziomie, można zastosować funckję \"ReLU\". Jeżeli natomiast mamy do dyspozycji wysokie zasoby obliczeniowe, warto pomyśleć nad zastosowaniem funkcji \"Sigmoid\". Funkcja \"Tanh\" jest czymś pomiędzy tymi dwoma.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Komentarze\n\n---\n\nPodczas wykonywania ćwiczenia laboratoryjnego natknąłem się na kilka nieścisłości w poleceniach. Przy zastosowaniu regularyzacji L2 nie był okreśony parametr \"Regularization rate\", więc wyszukałem i znalazłem pod [tym linkiem](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda), że pozostawienie go na zerze spowoduje nie zastosowanie regularyzacji w obliczeniach. Przy użyciu większych wartości strata oraz czas nauki były jeszcze większe, aż do momentu, gdy rozwiązanie problemu było niemożliwe - sieć nie mogła się nauczyć - przykładowo dla wartości \"Regularization rate\" równej \"1\". Mam więc nadzieję, że wartość \"0.01\" jest wystarczająca dla tego ćwiczenia.\n\nNie jestem też do końca pewny stwierdzenia \"kara dla nauczonej sieci\" z polecenia 3. Przyjąłem, że chodzi właśnie o parametr \"test loss\" i tak wykonałem ćwiczenie. Nie znalazłem w Internecie żadnej innej definicji.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}