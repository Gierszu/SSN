{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# **Laboratorium 2**: Tensorflow Playground\n*Wojciech Gierszal, 141037*\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Import\nW całym laboratorium będziemy korzystać z biblioteki numpy.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n## **Zadanie 1.**\nZaimplementuj prosty neuron realizujący dystans L2 dla wektora dwuelementowego (funkcja realizująca obliczenia w przód – feed-forward).\n\n---\n\nZa pomocą równania $f(x, W) = \\sum_{i = 1}^{n}(W \\cdot x)_i^2$ można stworzyć następującą funkcję:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def calculate_l2(inputs, weights):\n    return np.sum(np.dot(weights, inputs)**2)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Według przykładu podanego w laboratorium, możemy zapisać, że dla\n$\n\\mathbf{W} = \\begin{bmatrix}\n0.1 & 0.5 \\\\\n-0.3 & 0.8 \\\\\n\\end{bmatrix},\\quad\n\\mathbf{x} = \\begin{bmatrix}\n0.2 \\\\\n0.4 \\\\\n\\end{bmatrix}\n$\nwynikiem działania funkcji powinna być liczba $0.116$. Sprawdźmy zatem, czy otrzymujemy taki wynik:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "W = np.array([[0.1, 0.5], [-0.3, 0.8]])\nx = np.array([[0.2], [0.4]])\n\ncalculate_l2(x, W)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.11600000000000005"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Jak widać, wynik funkcji jest prawidłowy.\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Dla następujących zadań, jeżeli nie określono inaczej, używane są następujące parametry sieci:\n\n![Default Parameters](https://github.com/Gierszu/SSN/blob/master/lab2/images/param.png?raw=true)\n\nParametr \"Noise\" ustawiany jest albo na 0 albo na 25, a \"Features\" posiadają tylko \"X1\" oraz \"X2\":\n\n![Zero noise](https://github.com/Gierszu/SSN/blob/master/lab2/images/param_nonoise.png?raw=true)\n![25 noise](https://github.com/Gierszu/SSN/blob/master/lab2/images/param_noise.png?raw=true)\n![Features](https://github.com/Gierszu/SSN/blob/master/lab2/images/param_features.png?raw=true)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## **Zadanie 2.**\nDla wszystkich rodzajów danych (z wyjątkiem „Spiral”), sprawdź:\n\n### **2a.** \nCzy jest możliwe rozwiązanie zadania bez żadnej warstwy ukrytej. Badanie należy wykonać dla zaszumienia (Noise) równego 0 oraz 25. \n\n---\n\n* **Circle**: \n    Nie jest możliwe poprawne rozwiązanie zadania, niezależnie od poziomu zaszumienia.\n* **Exclusive Or**:\n    Nie jest możliwe poprawne rozwiązanie zadania, niezależnie od poziomu zaszumienia.\n* **Gaussian**:\n    Tak, dla poziomu zaszumienia równego 0, możliwe jest uzyskanie stuprocentowej pewności przy rozwiązaniu zadania.  \n    Dla poziomu zaszumienia równego 25, możliwe jest uzyskanie rozwiązania o bardzo wysokiej pewności:  \n    ![25 Noise Gaussian](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_gaussian.png?raw=true)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### **2b.** \nCzy jest możliwe rozwiązanie zadania z jedną warstwą ukrytą. Jaka jest minimalna liczba neuronów danej warstwy, która realizuje zadanie klasyfikacji? Badanie należy wykonać dla zaszumienia (Noise) równego 0 oraz 25. \n\n---\n\n* **Circle**: \n    Tak, ale wymagane są co najmniej trzy neurony warstwy ukrytej. Działa zarówno dla zaszumienia 0 i 25.\n* **Exclusive Or**:\n    Poprawną realizację określiłbym dla co najmniej czterech neuronów:  \n    ![XOR 4 Neurons](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor4.png?raw=true)\n    ![XOR 4 Neurons with noise](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor4noise.png?raw=true)  \n    Ciekawe wyniki pojawiały się natomiast dla dwóch neuronów. Sieć próbowała rozwiązać ten problem na zupełnie różne sposoby:  \n    ![XOR 2 Neurons 1](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor21.png?raw=true)\n    ![XOR 2 Neurons 2](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor22.png?raw=true)\n* **Gaussian**:\n    Tak, można uzyskać poprawną realizację nawet bez warstwy ukrytej, z warstwą ukrytą możemy zaobserowować jedynie niewielki wzrost dokładności.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### **2c.** oraz **2d.**\nCzy zastosowanie regularyzacji „L2” statystycznie polepsza jakość klasyfikacji, bądź przyśpiesza czas trenowania? Należy zestawić wyniki z tymi otrzymanymi w podpunkcie b.\n\n---\n\nObrazek po lewej dotyczy sieci bez regularyzacji, po prawej z regularyzacją L2. Wszystkie inne parametry są takie same.\n\n* **Circle**: \n    Nie ma zauważalnej zmiany.\n* **Exclusive Or**:\n    Poprawną realizację określiłbym dla co najmniej czterech neuronów:  \n    ![XOR 4 Neurons](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor4.png?raw=true)\n    ![XOR 4 Neurons with noise](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor4noise.png?raw=true)  \n    Ciekawe wyniki pojawiały się natomiast dla dwóch neuronów. Sieć próbowała rozwiązać ten problem na zupełnie różne sposoby:  \n    ![XOR 2 Neurons 1](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor21.png?raw=true)\n    ![XOR 2 Neurons 2](https://github.com/Gierszu/SSN/blob/master/lab2/images/result_xor22.png?raw=true)\n* **Gaussian**:\n    Tak, można uzyskać poprawną realizację nawet bez warstwy ukrytej, z warstwą ukrytą możemy zaobserowować jedynie niewielki wzrost dokładności.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}